{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f115e92",
   "metadata": {},
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7504ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "TEXT_DATA_PATH = \"tiny_shiekspear.txt\"\n",
    "with open(TEXT_DATA_PATH, \"r\") as file:\n",
    "    text_data = file.read()\n",
    "    \n",
    "print(text_data[:500])  # Print the first 500 characters of the text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6625b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text_data)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "print(f\"Unique characters: {''.join(chars)}\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9366c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(text):\n",
    "    return [stoi[c] for c in text]\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([itos[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71ed1c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [46, 43, 50, 50, 53, 2]\n",
      "Decoded: hello!\n"
     ]
    }
   ],
   "source": [
    "encoded = encode(\"hello!\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "decoded = decode(encoded)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa4f6129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to torch tensor of encoded text and print\n",
    "import torch\n",
    "\n",
    "train_data_tensor = torch.tensor(encode(text_data), dtype=torch.long)\n",
    "train_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1a70d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tensor[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c63aad80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b97e0b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and validation split\n",
    "TRAIN_SPLIT = 0.9\n",
    "train_data = train_data_tensor[:int(len(train_data_tensor)*0.9)]\n",
    "val_data = train_data_tensor[int(len(train_data_tensor)*0.9):]\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "644abf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block indices: [18, 47, 56, 57, 58, 1, 15, 47, 58]\n"
     ]
    }
   ],
   "source": [
    "# Box size = context window\n",
    "# visualize a block of text (size = block_size + 1)\n",
    "\n",
    "BLOCK_SIZE = 8\n",
    "block = train_data[:BLOCK_SIZE + 1]\n",
    "print(f\"Block indices: {block.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bd67222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For an input block: [18, 47, 56, 57, 58, 1, 15, 47]\n",
      "For input: [18], Next token would be: 47\n",
      "For input: [18, 47], Next token would be: 56\n",
      "For input: [18, 47, 56], Next token would be: 57\n",
      "For input: [18, 47, 56, 57], Next token would be: 58\n",
      "For input: [18, 47, 56, 57, 58], Next token would be: 1\n",
      "For input: [18, 47, 56, 57, 58, 1], Next token would be: 15\n",
      "For input: [18, 47, 56, 57, 58, 1, 15], Next token would be: 47\n",
      "For input: [18, 47, 56, 57, 58, 1, 15, 47], Next token would be: 58\n"
     ]
    }
   ],
   "source": [
    "# For one given block of text, we generate all possible continuations from the start\n",
    "# This gives us block_size examples that we can use to train the model\n",
    "\n",
    "X = train_data[: BLOCK_SIZE]\n",
    "y = train_data[1:BLOCK_SIZE+1] # y is just train data offset by one, as the task of the model is just to predict the next token given the previous sequence of tokens (or just last token in case of a bigram model)\n",
    "\n",
    "print(f\"For an input block: {X.tolist()}\")\n",
    "for t in range(BLOCK_SIZE):\n",
    "    print(f\"For input: {X[:t+1].tolist()}, Next token would be: {y[t]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62e186ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For an input block: [58, 43, 50, 50, 11, 0, 14, 59]\n",
      "For input: [58], Next token would be: 43\n",
      "For input: [58, 43], Next token would be: 50\n",
      "For input: [58, 43, 50], Next token would be: 50\n",
      "For input: [58, 43, 50, 50], Next token would be: 11\n",
      "For input: [58, 43, 50, 50, 11], Next token would be: 0\n",
      "For input: [58, 43, 50, 50, 11, 0], Next token would be: 14\n",
      "For input: [58, 43, 50, 50, 11, 0, 14], Next token would be: 59\n",
      "For input: [58, 43, 50, 50, 11, 0, 14, 59], Next token would be: 58\n"
     ]
    }
   ],
   "source": [
    "# To generate a batch of example tensor using the block code above:\n",
    "random_idx = torch.randint(0, len(train_data) - BLOCK_SIZE - 1, (1,)).item()\n",
    "X = train_data[random_idx: random_idx+BLOCK_SIZE]\n",
    "y = train_data[random_idx+1:random_idx+BLOCK_SIZE+1] # y is just train data offset by one, as the task of the model is just to predict the next token given the previous sequence of tokens (or just last token in case of a bigram model)\n",
    "\n",
    "print(f\"For an input block: {X.tolist()}\")\n",
    "for t in range(BLOCK_SIZE):\n",
    "    print(f\"For input: {X[:t+1].tolist()}, Next token would be: {y[t]}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84cd104f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 21,  1, 42, 53,  1, 58, 46],\n",
      "        [58,  1, 52, 53, 58,  1, 59, 57],\n",
      "        [43, 39, 50,  1, 42, 39, 47, 50],\n",
      "        [44,  1, 63, 53, 59,  1, 46, 43]])\n",
      "tensor([[21,  1, 42, 53,  1, 58, 46, 39],\n",
      "        [ 1, 52, 53, 58,  1, 59, 57,  1],\n",
      "        [39, 50,  1, 42, 39, 47, 50, 63],\n",
      "        [ 1, 63, 53, 59,  1, 46, 43, 50]])\n"
     ]
    }
   ],
   "source": [
    "# Stacking\n",
    "# To generate a batch of example tensor using the block code above:\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "indices = torch.randint(0, len(train_data) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n",
    "X = torch.stack([train_data[idx: idx+BLOCK_SIZE] for idx in indices])\n",
    "y = torch.stack([train_data[idx+1: idx+BLOCK_SIZE+1] for idx in indices])\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f11200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input tensor: tensor([1]), Target is: 21\n",
      "For input tensor: tensor([ 1, 21]), Target is: 1\n",
      "For input tensor: tensor([ 1, 21,  1]), Target is: 42\n",
      "For input tensor: tensor([ 1, 21,  1, 42]), Target is: 53\n",
      "For input tensor: tensor([ 1, 21,  1, 42, 53]), Target is: 1\n",
      "For input tensor: tensor([ 1, 21,  1, 42, 53,  1]), Target is: 58\n",
      "For input tensor: tensor([ 1, 21,  1, 42, 53,  1, 58]), Target is: 46\n",
      "For input tensor: tensor([ 1, 21,  1, 42, 53,  1, 58, 46]), Target is: 39\n",
      "For input tensor: tensor([58]), Target is: 1\n",
      "For input tensor: tensor([58,  1]), Target is: 52\n",
      "For input tensor: tensor([58,  1, 52]), Target is: 53\n",
      "For input tensor: tensor([58,  1, 52, 53]), Target is: 58\n",
      "For input tensor: tensor([58,  1, 52, 53, 58]), Target is: 1\n",
      "For input tensor: tensor([58,  1, 52, 53, 58,  1]), Target is: 59\n",
      "For input tensor: tensor([58,  1, 52, 53, 58,  1, 59]), Target is: 57\n",
      "For input tensor: tensor([58,  1, 52, 53, 58,  1, 59, 57]), Target is: 1\n",
      "For input tensor: tensor([43]), Target is: 39\n",
      "For input tensor: tensor([43, 39]), Target is: 50\n",
      "For input tensor: tensor([43, 39, 50]), Target is: 1\n",
      "For input tensor: tensor([43, 39, 50,  1]), Target is: 42\n",
      "For input tensor: tensor([43, 39, 50,  1, 42]), Target is: 39\n",
      "For input tensor: tensor([43, 39, 50,  1, 42, 39]), Target is: 47\n",
      "For input tensor: tensor([43, 39, 50,  1, 42, 39, 47]), Target is: 50\n",
      "For input tensor: tensor([43, 39, 50,  1, 42, 39, 47, 50]), Target is: 63\n",
      "For input tensor: tensor([44]), Target is: 1\n",
      "For input tensor: tensor([44,  1]), Target is: 63\n",
      "For input tensor: tensor([44,  1, 63]), Target is: 53\n",
      "For input tensor: tensor([44,  1, 63, 53]), Target is: 59\n",
      "For input tensor: tensor([44,  1, 63, 53, 59]), Target is: 1\n",
      "For input tensor: tensor([44,  1, 63, 53, 59,  1]), Target is: 46\n",
      "For input tensor: tensor([44,  1, 63, 53, 59,  1, 46]), Target is: 43\n",
      "For input tensor: tensor([44,  1, 63, 53, 59,  1, 46, 43]), Target is: 50\n"
     ]
    }
   ],
   "source": [
    "# This X and y batch are both (B,T) matrices, \n",
    "#   where the B dim is the blocks across a BATCH\n",
    "#   T dim is across a block through TIME\n",
    "# For any given block Bx, for any given sequence Bx[start:end], the next token in the sequence would be By[end]\n",
    "for block in range(BATCH_SIZE):\n",
    "    for t in range(BLOCK_SIZE):\n",
    "        print(f\"For input tensor: {X[block, :t+1]}, Target is: {y[block, t]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6afaa733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[32, 21, 27, 10,  0, 13, 56, 43],\n",
       "         [39, 57, 58, 47, 51, 43,  1, 58],\n",
       "         [23, 17,  1, 27, 18,  1, 37, 27],\n",
       "         [ 5,  1, 59, 52, 58, 53,  1, 58]]),\n",
       " tensor([[21, 27, 10,  0, 13, 56, 43,  1],\n",
       "         [57, 58, 47, 51, 43,  1, 58, 53],\n",
       "         [17,  1, 27, 18,  1, 37, 27, 30],\n",
       "         [ 1, 59, 52, 58, 53,  1, 58, 46]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get batch -> x, y for a random batch (of blocks)\n",
    "# Note 1\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    indices = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    X = torch.stack([data[idx: idx+block_size] for idx in indices])\n",
    "    y = torch.stack([data[idx+1: idx+block_size+1] for idx in indices])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce21b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram language model that uses nn.Embedding to generate Logits, cross entropy loss ### Comeback and try making a trigram model too\n",
    "# Check dimenstionality of input and output\n",
    "# Note 2\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_lookup_table = nn.Embedding(input_vocab_size, output_vocab_size)\n",
    "\n",
    "        # nn.embedding is essentially a matrix,\n",
    "        # I don't know how it works\n",
    "        # But here's my guess\n",
    "        # It is a (input_values, output_values) matrix, where the input token equals the possible input variation\n",
    "        # In a bigram model, where we look at one past character to predict the next one, input_values = output_values = charater_set_length\n",
    "        # In a trigram model, where we look at two past characters to predict on next character, output_value = charater_set_length, input_values = charater_set_length ** 2\n",
    "        # In this trigram model, the embedding matric would me (charater_set_length ** 2, charater_set_length)\n",
    "        # In LLM, I am guessing this is a (vocab_size, vocab_size) matrix\n",
    "        \n",
    "        \n",
    "        # Now as to what it does, the forward pass using this as a lookup table of probabilities,\n",
    "        # For any given row, we treat the column values as the probabilities of which output token should come next\n",
    "        # This way when we back prop, we are optimizing for the lookup table to resemble a probability distribution matrix as done is makemore\n",
    "        \n",
    "        # Apparently I am precisely correct about how nn.Embedding does. This might as well have been implemented with a normal (input_values, output_values) Tensor with gradients and it would've worked just the same\n",
    "\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        # x is shape (B, T), where B is batch size, T is time steps (or block size)\n",
    "        x = self.embedding_lookup_table(idx)\n",
    "        # x is now (B, T, C) where C is the character set size (vocab size)\n",
    "        if not target:\n",
    "            return x, None\n",
    "        return x , F.cross_entropy()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "819d0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the forward pass to work, cross_entropy expects input, target. BUTTTTT Andrej mentioned something the input from B,T,C to B,C, T, will run and test\n",
    "model = BigramLanguageModel(VOCAB_SIZE, VOCAB_SIZE)\n",
    "\n",
    "X, y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19255831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 65])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass one token, which is the last token of the input sequence, because this is a bigram model\n",
    "# But the input sequnce in a moving window, as we printed above\n",
    "# So the actual lookup indices should be a (B*BLOCK_SIZE, C)\n",
    "# But I don't know how to implement that, so I am justs going to get one example per block for the sack of making the forward pass work\n",
    "\n",
    "print(model.embedding_lookup_table(X[:, -1]).shape), print(y[:, -1].shape)\n",
    "# This gives me (BATCH_SIZE, VOCAB_SIZE), where the 65 is the vocab size logtis that I am assuming are the probabilites of what the next token should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e65dc232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.distributions.Polynomial() # I don't know how this will work in pytorch, but I am guessing after we get the embedding logits,\n",
    "# We we sample for a distribution of that embedding, which will give us the index of the next token, that will be our actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0100a1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7363, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = model.embedding_lookup_table(X[:, -1])\n",
    "target = y[:, -1]\n",
    "F.cross_entropy(seq, target)\n",
    "\n",
    "# This works, but again, this is (B, C) against (B), I have no idea how to make this work in (B, T, C) against (B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4632ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oh, I keep struggling because I keep trying to write the most general solution ever\n",
    "# This is a bigram model, model.embedding_lookup_table(X) for a (B, T) matrix will give me (B, T, C). This is because in bigram models, only the current token is needed to predict the next token\n",
    "# No need to do the whole generate completel sequence thing. This will not work for Trigram, or atleast not exactly like this\n",
    "print(model.embedding_lookup_table(X).shape), print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5cd67fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 65], got [4, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m seq = model.embedding_lookup_table(X)\n\u001b[32m      2\u001b[39m target = y\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# F.cross_entropy(seq, target) given an error, RuntimeError: Expected target size [4, 65], got [4, 8]\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# I am guessing this is because, my target is indices, not probablity values to match with. So cross entropy thinks that in my input (B, T, C). T is the probabilities, and C is the classes.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# essentially (batch, probability, class)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# So for this I will transform seq but only the two inner dimensions, so that is goes from (B, T, C) to (B, C, T)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github_repos_windows\\llm-finetuning-review\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected target size [4, 65], got [4, 8]"
     ]
    }
   ],
   "source": [
    "seq = model.embedding_lookup_table(X)\n",
    "target = y\n",
    "F.cross_entropy(seq, target)\n",
    "\n",
    "# F.cross_entropy(seq, target) given an error, RuntimeError: Expected target size [4, 65], got [4, 8]\n",
    "# I am guessing this is because, my target is indices, not probablity values to match with. So cross entropy thinks that in my input (B, T, C). T is the probabilities, and C is the classes.\n",
    "# essentially (batch, probability, class)\n",
    "# So for this I will transform seq but only the two inner dimensions, so that is goes from (B, T, C) to (B, C, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 65, 8])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.transpose(-2, -1).shape\n",
    "# Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f0cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8080, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(seq.transpose(-2, -1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24363c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boom!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f404cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), torch.Size([]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_lookup_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        # This is how I did it:\n",
    "        # x = self.embedding_lookup_table(idx)\n",
    "        # if target is None:\n",
    "        #     return x, None\n",
    "        # return x , F.cross_entropy(x.transpose(-2, -1), target)\n",
    "\n",
    "        # this is how Andrej did it\n",
    "        logits = self.embedding_lookup_table(idx)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = target.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def my_forward(self, idx, target=None):\n",
    "        x = self.embedding_lookup_table(idx)\n",
    "        if target is None:\n",
    "            return x, None\n",
    "        return x , F.cross_entropy(x.transpose(-2, -1), target)\n",
    "\n",
    "X, y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "model = BigramLanguageModel(VOCAB_SIZE)\n",
    "y_pred , loss = model.forward(X, y)\n",
    "y_pred.shape, loss\n",
    "\n",
    "# Well my and Andrej's methods are giving different results.... welp lemme chatgpt what is the differenct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfea00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), tensor(4.6674, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred , loss = model.forward(X, y)\n",
    "y_pred.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef19755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 65]), tensor(4.6674, grad_fn=<NllLoss2DBackward0>))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred , loss = model.my_forward(X, y)\n",
    "y_pred.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc287871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea what changed (Edit: I do now, different batch in the first run), but by implementing them both, very clearly they are matehmatically equivalent\n",
    "# I'm a genius!\n",
    "# Look Ma! No hands!\n",
    "# Now let's cleanup, and implement a max_token limited generate function to let the model babble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051b684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 65]), tensor(4.3515, grad_fn=<NllLoss2DBackward0>))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_lookup_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        x = self.embedding_lookup_table(idx)\n",
    "        if target is None:\n",
    "            return x, None\n",
    "        return x , F.cross_entropy(x.transpose(-2, -1), target)\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): # idx is the start seed token\n",
    "        # idx is [B, T], ideally where T=1, so a batch of input seed tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx) # Generate logits for what should come next for each B\n",
    "            logits = logits[:, -1,:]\n",
    "            probs = F.softmax(logits, dim=1) # We softmax the logits to convert them into probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # We pick the next token from a distribution \n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # We concatenate the newly generated token to the end of the starting sequence\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "X, y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "model = BigramLanguageModel(VOCAB_SIZE)\n",
    "y_pred , loss = model.forward(X, y)\n",
    "y_pred.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057aaa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hm!pBQGR\n",
      "h:\n",
      "kqX3I!oMbB&TSsDMIizI!rF\n",
      "xjpAwI-Ry.GjLGwatYyBquxjFOBYZeOEYFObrQOB&Lr-fK&tu-yTeB3U,SfnjGYF&'GN!$OBTZ 3V hfoQOBTpd'tpA-&w-ctxbmDdW:W$I'cHdlfPJolw!Ii.CnuB!ooCuWoqM?R-!B.anOB3o$pNLjYF'EEYUtPnM;OwNEJSdkMt3u'Z;OBLTSFNDk:v,;&fk:vpvrhiEAx.ddoyc!jAy.'tRY;OBky $Vv;UpdjMtxSuTthbaTH oZhy!?LQY;OMc ;OB!Ikgx\n",
      "&TOBvEoB&FKoleMGYVK\n",
      "G:gPb-yGthAqfoU,OBTZelR&OM?t&MUvLqN'vvbsv.Gs;hUNJ\n",
      "ZTlSBzgpH!J&DS&U,qUvFAo3ErGgjai.\n",
      "v SjfI3Id;,sDYfPxVQF'.yd'JSuk:wICpO BOJApdPOgwjM?W:t'Qy.&hank:VXbPMI!Wsm-,Krhcl?VY$aUu?ulyo&g'P,Lin3JGjdXYXNE$lfTI'Q;SuvlSdBu!glbf'w;jGR3MlwmbMbBOltzVyA,SB-BO;vJ3ILrypDkAelgD'fo &:.FVnD:anjrhnM3lw!,,weuVWgKoRRqahynDYEQTy sd-lw3$fZfjCux?qdkZm!-sszzmTRvvi$n;\n",
      "Zx\n",
      "Q?iRPahuQ;OBgxwevAbNkYFZuF!m!vb\n",
      ";OBhEOB3Iu tZtuDeYYZfr-GtwbaqfgMZANkazuVXYFsvv.FPFIV'wNEiSXBth,Bvm,tD.PRyUbowI$NSPHD\n",
      ";cK\n",
      "Rq-' nCHvb' xXpavJ$r-ouA3JYWbMGAHe' yLNYUc\n",
      "khcscsPnBHminrh,SLQTE&tZnNEoe?QFjrfoM;FsV3tzgX:;OBYkAYByNE;\n",
      "Wk!AH$qvbIdXAU\n",
      "$z.EAYvbE'hiSPKNkkgqpe'tsxqZbGv,Sdg:gvTbOUbTLyGfYxFTlxpELw?EYesFZOcMdiY3lDJVeSuN'eYFrF,wftfi\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros(1, 1, dtype=torch.long), 1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387120e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rubbish, but expected because the model is untrained. Let's train!\n",
    "model = BigramLanguageModel(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c10f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4207818508148193\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch(train_data, batch_size, BLOCK_SIZE)\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " m fo, g thr.\n",
      "DWhirdrind Vaxce? fathe ar ie VEThe fo in my: Whe GEENESSThat\n",
      "D:\n",
      "Ithay ee s e:\n",
      "Singlithe angrs,\n",
      "Yorou\n",
      "\n",
      "GHarierel magho than uryootofrdin, m me e blofonof.\n",
      "Thers onk thought thearessk, nang le;\n",
      "Finceilerisbu orth I fer s thistheaven.\n",
      "A:\n",
      "Waigsuralairarwhinomanstt se fongheenon fr. d t berve'eck y loueldind hirelaimucknar I thoupoy N:\n",
      "\n",
      "Yo umy sw,\n",
      "thaecty yo won, alirshigorthe storirn hinto 'lise hbbouttlthe,\n",
      "USh-the?\n",
      "An sthe ly.\n",
      "\n",
      "BULTerin Wes p thonwaltovouberon f siery sirofodsucthe dre ows y ot then.\n",
      "uterorthiviggobour an.\n",
      "\n",
      "\n",
      "Shasifoofoinoutitins hindi'd hin iusithathaped fullalofo, he'sthote y'ORINGHalle bu hech ten Gllilst my, marron'shehee,\n",
      "Priate theses t yowich ve lerenerer t pue\n",
      "\n",
      "\n",
      "ID nwac foushendorof home th at.\n",
      "ARO ear gin bl hese,\n",
      "FOFiree\n",
      "\n",
      "ABrine llo knge Fob ashiabers tin'sowh ftof o, M:\n",
      "Helo lthese ISe:\n",
      "Ork theth hay t:\n",
      "YOfren foend horsofod.\n",
      "Tit cr or d d pun:\n",
      "METINRKERDanngeawaclita nn'su orghad I waighefin!\n",
      "NGENond 'lar ougorir my ke yor\n",
      "COLONTotho te d,\n",
      "QUS:\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.ones(1, 1, dtype=torch.long), 1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good enough, let'smove this over to transformer_dev.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d5adc",
   "metadata": {},
   "source": [
    "# Trigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "15460d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Parameters\n",
    "TEXT_DATA_PATH = \"tiny_shiekspear.txt\"\n",
    "VOCAB_SIZE = 65\n",
    "TRAIN_SPLIT = 0.9\n",
    "BLOCK_SIZE = 8\n",
    "BATCH_SIZE = 64\n",
    "MAX_NEW_TOKENS = 1000\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "with open(TEXT_DATA_PATH, \"r\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "chars = sorted(list(set(text_data)))\n",
    "# Also add all character pairs into the encoder and decoder to be able to make a trigram language model\n",
    "vocab = [a+b for a in chars for b in chars]\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "input_vocab_size = len(vocab)\n",
    "output_vocab_size = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e6d8b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    return [stoi[text[idx : idx + 2]] for idx in range(len(text) - 1)]\n",
    "\n",
    "def decode(indices):\n",
    "    output = [itos[indices[0]]]\n",
    "    output += [itos[idx][1] for idx in indices[1:]]\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e3806e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1343, 2845, 3300, 3303]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "27d8cbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "49dacb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115393])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to torch tensor of encoded text and print\n",
    "import torch\n",
    "\n",
    "train_data_tensor = torch.tensor(encode(text_data), dtype=torch.long)\n",
    "train_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a4f2cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1217, 3111, 3697, 3763, 3771,   80, 1022, 3113, 3817, 3119, 4203, 2847,\n",
       "        3390,  650,   14,  953, 2839, 2913, 3501, 3683, 2796,  126, 4008, 2796,\n",
       "         119, 3566, 3693, 3486, 2708, 2838, 2837, 2731,  104, 2587, 3443, 4096,\n",
       "         109, 2919, 3891, 3698, 3816, 3033, 2851, 3646,  391,  111, 3033, 2834,\n",
       "        2591, 3641,  116, 3358, 2796,  122, 3759, 3553, 2834, 2584, 3193,  520,\n",
       "           0,   13,  895, 3300, 3260,  650,   31, 2069, 3553, 2834, 2584, 3191,\n",
       "         391,  122, 3759, 3553, 2834, 2584, 3193,  520,    0,   18, 1217, 3111,\n",
       "        3697, 3763, 3771,   80, 1022, 3113, 3817, 3119, 4203, 2847, 3390,  650,\n",
       "          37, 2458, 3504, 3836])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tensor[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f09f76ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8fb81be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003853]), torch.Size([111540]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and validation split\n",
    "TRAIN_SPLIT = 0.9\n",
    "train_data = train_data_tensor[:int(len(train_data_tensor)*0.9)]\n",
    "val_data = train_data_tensor[int(len(train_data_tensor)*0.9):]\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7e0b86bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block indices: [1217, 3111, 3697, 3763, 3771, 80, 1022, 3113, 3817]\n"
     ]
    }
   ],
   "source": [
    "# Box size = context window\n",
    "# visualize a block of text (size = block_size + 1)\n",
    "\n",
    "BLOCK_SIZE = 8\n",
    "block = train_data[:BLOCK_SIZE + 1]\n",
    "print(f\"Block indices: {block.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3611590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For an input block: [1217, 3111, 3697, 3763, 3771, 80, 1022, 3113]\n",
      "For input: [1217], Next token would be: 3111\n",
      "For input: [1217, 3111], Next token would be: 3697\n",
      "For input: [1217, 3111, 3697], Next token would be: 3763\n",
      "For input: [1217, 3111, 3697, 3763], Next token would be: 3771\n",
      "For input: [1217, 3111, 3697, 3763, 3771], Next token would be: 80\n",
      "For input: [1217, 3111, 3697, 3763, 3771, 80], Next token would be: 1022\n",
      "For input: [1217, 3111, 3697, 3763, 3771, 80, 1022], Next token would be: 3113\n",
      "For input: [1217, 3111, 3697, 3763, 3771, 80, 1022, 3113], Next token would be: 3817\n"
     ]
    }
   ],
   "source": [
    "# For one given block of text, we generate all possible continuations from the start\n",
    "# This gives us block_size examples that we can use to train the model\n",
    "\n",
    "X = train_data[: BLOCK_SIZE]\n",
    "y = train_data[1:BLOCK_SIZE+1] # y is just train data offset by one, as the task of the model is just to predict the next token given the previous sequence of tokens (or just last token in case of a bigram model)\n",
    "\n",
    "print(f\"For an input block: {X.tolist()}\")\n",
    "for t in range(BLOCK_SIZE):\n",
    "    print(f\"For input: {X[:t+1].tolist()}, Next token would be: {y[t]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1212593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For an input block: [2847, 3381, 126, 4008, 2838, 2849, 3557, 3107]\n",
      "For input: [2847], Next token would be: 3381\n",
      "For input: [2847, 3381], Next token would be: 126\n",
      "For input: [2847, 3381, 126], Next token would be: 4008\n",
      "For input: [2847, 3381, 126, 4008], Next token would be: 2838\n",
      "For input: [2847, 3381, 126, 4008, 2838], Next token would be: 2849\n",
      "For input: [2847, 3381, 126, 4008, 2838, 2849], Next token would be: 3557\n",
      "For input: [2847, 3381, 126, 4008, 2838, 2849, 3557], Next token would be: 3107\n",
      "For input: [2847, 3381, 126, 4008, 2838, 2849, 3557, 3107], Next token would be: 3425\n"
     ]
    }
   ],
   "source": [
    "# To generate a batch of example tensor using the block code above:\n",
    "random_idx = torch.randint(0, len(train_data) - BLOCK_SIZE - 1, (1,)).item()\n",
    "X = train_data[random_idx: random_idx+BLOCK_SIZE]\n",
    "y = train_data[random_idx+1:random_idx+BLOCK_SIZE+1] # y is just train data offset by one, as the task of the model is just to predict the next token given the previous sequence of tokens (or just last token in case of a bigram model)\n",
    "\n",
    "print(f\"For an input block: {X.tolist()}\")\n",
    "for t in range(BLOCK_SIZE):\n",
    "    print(f\"For input: {X[:t+1].tolist()}, Next token would be: {y[t]}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b9235e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2796,  111, 3043, 3496, 3358, 2803,  520,    0],\n",
      "        [3816, 3029, 2593, 3771,  128, 4148, 3504, 3836],\n",
      "        [3228, 2796,  123, 3823, 3446,  111, 3037, 3106],\n",
      "        [2851, 3683, 2795,   52, 3433, 3497, 3423, 2796]])\n",
      "tensor([[ 111, 3043, 3496, 3358, 2803,  520,    0,   13],\n",
      "        [3029, 2593, 3771,  128, 4148, 3504, 3836,  123],\n",
      "        [2796,  123, 3823, 3446,  111, 3037, 3106, 3327],\n",
      "        [3683, 2795,   52, 3433, 3497, 3423, 2796,  118]])\n"
     ]
    }
   ],
   "source": [
    "# Stacking\n",
    "# To generate a batch of example tensor using the block code above:\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "indices = torch.randint(0, len(train_data) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n",
    "X = torch.stack([train_data[idx: idx+BLOCK_SIZE] for idx in indices])\n",
    "y = torch.stack([train_data[idx+1: idx+BLOCK_SIZE+1] for idx in indices])\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "391637af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input tensor: tensor([2796]), Target is: 111\n",
      "For input tensor: tensor([2796,  111]), Target is: 3043\n",
      "For input tensor: tensor([2796,  111, 3043]), Target is: 3496\n",
      "For input tensor: tensor([2796,  111, 3043, 3496]), Target is: 3358\n",
      "For input tensor: tensor([2796,  111, 3043, 3496, 3358]), Target is: 2803\n",
      "For input tensor: tensor([2796,  111, 3043, 3496, 3358, 2803]), Target is: 520\n",
      "For input tensor: tensor([2796,  111, 3043, 3496, 3358, 2803,  520]), Target is: 0\n",
      "For input tensor: tensor([2796,  111, 3043, 3496, 3358, 2803,  520,    0]), Target is: 13\n",
      "For input tensor: tensor([3816]), Target is: 3029\n",
      "For input tensor: tensor([3816, 3029]), Target is: 2593\n",
      "For input tensor: tensor([3816, 3029, 2593]), Target is: 3771\n",
      "For input tensor: tensor([3816, 3029, 2593, 3771]), Target is: 128\n",
      "For input tensor: tensor([3816, 3029, 2593, 3771,  128]), Target is: 4148\n",
      "For input tensor: tensor([3816, 3029, 2593, 3771,  128, 4148]), Target is: 3504\n",
      "For input tensor: tensor([3816, 3029, 2593, 3771,  128, 4148, 3504]), Target is: 3836\n",
      "For input tensor: tensor([3816, 3029, 2593, 3771,  128, 4148, 3504, 3836]), Target is: 123\n",
      "For input tensor: tensor([3228]), Target is: 2796\n",
      "For input tensor: tensor([3228, 2796]), Target is: 123\n",
      "For input tensor: tensor([3228, 2796,  123]), Target is: 3823\n",
      "For input tensor: tensor([3228, 2796,  123, 3823]), Target is: 3446\n",
      "For input tensor: tensor([3228, 2796,  123, 3823, 3446]), Target is: 111\n",
      "For input tensor: tensor([3228, 2796,  123, 3823, 3446,  111]), Target is: 3037\n",
      "For input tensor: tensor([3228, 2796,  123, 3823, 3446,  111, 3037]), Target is: 3106\n",
      "For input tensor: tensor([3228, 2796,  123, 3823, 3446,  111, 3037, 3106]), Target is: 3327\n",
      "For input tensor: tensor([2851]), Target is: 3683\n",
      "For input tensor: tensor([2851, 3683]), Target is: 2795\n",
      "For input tensor: tensor([2851, 3683, 2795]), Target is: 52\n",
      "For input tensor: tensor([2851, 3683, 2795,   52]), Target is: 3433\n",
      "For input tensor: tensor([2851, 3683, 2795,   52, 3433]), Target is: 3497\n",
      "For input tensor: tensor([2851, 3683, 2795,   52, 3433, 3497]), Target is: 3423\n",
      "For input tensor: tensor([2851, 3683, 2795,   52, 3433, 3497, 3423]), Target is: 2796\n",
      "For input tensor: tensor([2851, 3683, 2795,   52, 3433, 3497, 3423, 2796]), Target is: 118\n"
     ]
    }
   ],
   "source": [
    "# This X and y batch are both (B,T) matrices, \n",
    "#   where the B dim is the blocks across a BATCH\n",
    "#   T dim is across a block through TIME\n",
    "# For any given block Bx, for any given sequence Bx[start:end], the next token in the sequence would be By[end]\n",
    "for block in range(BATCH_SIZE):\n",
    "    for t in range(BLOCK_SIZE):\n",
    "        print(f\"For input tensor: {X[block, :t+1]}, Target is: {y[block, t]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5573310c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3107, 3425, 2926,  123, 3816, 3046, 3693, 3504],\n",
       "         [2585, 3300, 3251,   86, 1366,  105, 2656, 3687],\n",
       "         [2837, 2731,  118, 3497, 3421, 2708, 2801,  391],\n",
       "         [2097, 1135, 1960,  650,   35, 2321, 3029, 2593]]),\n",
       " tensor([[3425, 2926,  123, 3816, 3046, 3693, 3504, 3880],\n",
       "         [3300, 3251,   86, 1366,  105, 2656, 3687, 3107],\n",
       "         [2731,  118, 3497, 3421, 2708, 2801,  391,  126],\n",
       "         [1135, 1960,  650,   35, 2321, 3029, 2593, 3776]]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get batch -> x, y for a random batch (of blocks)\n",
    "# Note 1\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    indices = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    X = torch.stack([data[idx: idx+block_size] for idx in indices])\n",
    "    y = torch.stack([data[idx+1: idx+block_size+1] for idx in indices])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "59c1aa5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 4225]), tensor(8.8089, grad_fn=<NllLoss2DBackward0>))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TrigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_lookup_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        x = self.embedding_lookup_table(idx)\n",
    "        if target is None:\n",
    "            return x, None\n",
    "        return x , F.cross_entropy(x.transpose(-2, -1), target)\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): # idx is the start seed token\n",
    "        # idx is [B, T], ideally where T=1, so a batch of input seed tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx) # Generate logits for what should come next for each B\n",
    "            logits = logits[:, -1,:]\n",
    "            probs = F.softmax(logits, dim=1) # We softmax the logits to convert them into probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # We pick the next token from a distribution \n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # We concatenate the newly generated token to the end of the starting sequence\n",
    "        return idx\n",
    "\n",
    "X, y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "model = TrigramLanguageModel(len(vocab))\n",
    "y_pred , loss = model.forward(X, y)\n",
    "y_pred.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "528e4fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e6b53528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ZuHxQPIrzla3AYLWOXSCapdF!UzvBMZ,rNpsJjfKEMSmqeX&fPX$QVeuR?vySE gpjN,QnP&EoVRlLKCltczxPIq-.WgLYCx NlN\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros(1,1, dtype=torch.long), max_new_tokens=100).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d799aaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "550cf0fd",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "967f3a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "\n",
    "#x = torch.randn(B, T, C)\n",
    "x = torch.randint(0, 10, (B, T, C)).float()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c998fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass informtion from (T-1), (T-2)... (T-N) for any given Token (T), such that no information diffuse from future tokens to current token\n",
    "xbow = torch.zeros((B,T,C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xbow_prev = x[b, :t+1]\n",
    "        xbow[b,t] = torch.mean(xbow_prev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7d06d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 4.],\n",
       "        [1., 1.],\n",
       "        [0., 7.],\n",
       "        [1., 4.],\n",
       "        [1., 3.],\n",
       "        [5., 3.],\n",
       "        [8., 4.],\n",
       "        [8., 4.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1770cc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.0000, 4.0000],\n",
       "        [4.0000, 2.5000],\n",
       "        [2.6667, 4.0000],\n",
       "        [2.2500, 4.0000],\n",
       "        [2.0000, 3.8000],\n",
       "        [2.5000, 3.6667],\n",
       "        [3.2857, 3.7143],\n",
       "        [3.8750, 3.7500]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e4fb6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  4.],\n",
       "        [ 8.,  5.],\n",
       "        [ 8., 12.],\n",
       "        [ 9., 16.],\n",
       "        [10., 19.],\n",
       "        [15., 22.],\n",
       "        [23., 26.],\n",
       "        [31., 30.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But that nested for loop is very slow in python. So we need some matrixy trick to get there.\n",
    "# Using a lower triangular matrix of ones, multiplied with our original matrix x, gives us cumulative sums going down the T dim\n",
    "\n",
    "wei =torch.tril(torch.ones(T, T))\n",
    "print(wei)\n",
    "\n",
    "(wei @ x)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fe87440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8., 7., 6., 5., 4., 3., 2., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.sum(dim = 0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9565c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to get cumulaive averages instead of cum-sums, we just need to normalize the tril matrix across rows\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c54b83ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.0000, 4.0000],\n",
       "        [4.0000, 2.5000],\n",
       "        [2.6667, 4.0000],\n",
       "        [2.2500, 4.0000],\n",
       "        [2.0000, 3.8000],\n",
       "        [2.5000, 3.6667],\n",
       "        [3.2857, 3.7143],\n",
       "        [3.8750, 3.7500]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wei @ x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cafc26d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.0000, 4.0000],\n",
       "        [4.0000, 2.5000],\n",
       "        [2.6667, 4.0000],\n",
       "        [2.2500, 4.0000],\n",
       "        [2.0000, 3.8000],\n",
       "        [2.5000, 3.6667],\n",
       "        [3.2857, 3.7143],\n",
       "        [3.8750, 3.7500]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce59ae28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# works!\n",
    "# more emperically\n",
    "torch.allclose(wei @ x, xbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31af240f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.0000, 4.0000],\n",
       "        [4.0000, 2.5000],\n",
       "        [2.6667, 4.0000],\n",
       "        [2.2500, 4.0000],\n",
       "        [2.0000, 3.8000],\n",
       "        [2.5000, 3.6667],\n",
       "        [3.2857, 3.7143],\n",
       "        [3.8750, 3.7500]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another method to do this is, using softmax\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim = 1)\n",
    "(wei @ x)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b218c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Positional embeddings\\nBefore implementing self attention, a few things need to be imlemented\\n1. In the bigram model, the embedding module is being used as a probability lookup table. It should be used as an embedding layer, where the embedding output \\n    is a feature vector of length embedding_length, which a linear layer than converts into vocab size logits. This vector will represent what a certain vector means.\\n2. We will create a second embedding module of size (block_size, embedding length). This will be used as position embedding, so it gives us the same vector for a certain position of \\n    a token, regardless of its value. This vector will represent where in a sentence does it exist.\\n3. We will Add both of these vectors togethere before passing them to the linear layer.\\n\\n\\n# Self Attention:\\nEvery token will emit two vector: Query and key\\n1. Query represents what I am looking for\\n2. Key represent what do I contain\\n\\nFor any given token T, We will dot product the query[T] with the key of all other tokens. \\nIn case of an encoder, this will be query[T] * (key[:T] and key[T+1:])\\nIn case of a decoder, we donot leak information from future tokens, so for that query[T] * key[:T]\\nDuring training, this decoder quirk will be implementeed using the tril technique above.\\n\\nSOOOO to summarize:\\n(B, T) is the input matrix, where a \\nx = positions_embeddings + token_embeddings\\nwei = \\n\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Positional embeddings\n",
    "Before implementing self attention, a few things need to be imlemented\n",
    "1. In the bigram model, the embedding module is being used as a probability lookup table. It should be used as an embedding layer, where the embedding output \n",
    "    is a feature vector of length embedding_length, which a linear layer than converts into vocab size logits. This vector will represent what a certain vector means.\n",
    "2. We will create a second embedding module of size (block_size, embedding length). This will be used as position embedding, so it gives us the same vector for a certain position of \n",
    "    a token, regardless of its value. This vector will represent where in a sentence does it exist.\n",
    "3. We will Add both of these vectors togethere before passing them to the linear layer.\n",
    "\n",
    "\n",
    "# Self Attention:\n",
    "Every token will emit two vector: Query and key\n",
    "1. Query represents what I am looking for\n",
    "2. Key represent what do I contain\n",
    "\n",
    "For any given token T, We will dot product the query[T] with the key of all other tokens. \n",
    "In case of an encoder, this will be query[T] * (key[:T] and key[T+1:])\n",
    "In case of a decoder, we donot leak information from future tokens, so for that query[T] * key[:T]\n",
    "During training, this decoder quirk will be implementeed using the tril technique above.\n",
    "\n",
    "SOOOO to summarize:\n",
    "(B, T) is the input matrix, where a B row is a block and a T column is a token\n",
    "For a given T token,\n",
    "x = positions_embeddings[T] + token_embeddings[T]\n",
    "wei = query[T] _dot_product_ key[:T]\n",
    "wei = trill_masking(wei)\n",
    "\n",
    "y = wei @ x\n",
    "Which we then pass to a Linear layer\n",
    "output = Softmax(y_linear)\n",
    "\n",
    "SOOO to summarize. For Self Attention, for any given token, to predict the next token, I need to know:\n",
    "1. What this token represents (Token Embeddings)\n",
    "2. Where in a block this token resides (Position Embeddings)\n",
    "3. What this token needs from all other tokens (Query)\n",
    "4. What all other tokens before T have (keys[:T])\n",
    "5. What this token will communicate if other token find it interesting (values[T])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d7dcdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, let's implement one self-attention head\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Attention head\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = key(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim = 1)\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2a2bc8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, Adding value linear layer too instead of using the raw x values\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Attention head\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, 65, bias=False)\n",
    "k = query(x) # (B, T, head_size)\n",
    "q = key(x) # (B, T, head_size)\n",
    "v = value(x)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  * C **-0.5 # (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril[:T, :T] == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3f0f955a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b61a5cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "806a3117",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x16 and 32x65)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[200]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m65\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github_repos_windows\\llm-finetuning-review\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github_repos_windows\\llm-finetuning-review\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github_repos_windows\\llm-finetuning-review\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (32x16 and 32x65)"
     ]
    }
   ],
   "source": [
    "nn.Linear(32, 65)(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "16e79ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2143, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1700a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Parameters\n",
    "TEXT_DATA_PATH = \"tiny_shiekspear.txt\"\n",
    "TRAIN_SPLIT = 0.9\n",
    "VOCAB_SIZE = 65\n",
    "BLOCK_SIZE = 8\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "N_EMBED = 32\n",
    "HEAD_SIZE = 16\n",
    "\n",
    "EPOCHS = 30000\n",
    "EVAL_ITERS = 100\n",
    "EVAL_INTERVAL = 250\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "MAX_NEW_TOKENS = 1000\n",
    "\n",
    "#DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "with open(TEXT_DATA_PATH, \"r\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "chars = sorted(list(set(text_data)))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    return [stoi[c] for c in text]\n",
    "\n",
    "\n",
    "def decode(indices):\n",
    "    return \"\".join([itos[i] for i in indices])\n",
    "\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    indices = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    X = torch.stack([data[idx : idx + block_size] for idx in indices])\n",
    "    y = torch.stack([data[idx + 1 : idx + block_size + 1] for idx in indices])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module, eval_iters, train_data, val_data):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, y = (\n",
    "                get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "                if split == \"train\"\n",
    "                else get_batch(val_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "            )\n",
    "            logits, loss = model(X, y)\n",
    "\n",
    "            losses[k] = loss\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "train_data_tensor = torch.tensor(encode(text_data), dtype=torch.long).to(DEVICE)\n",
    "train_data = train_data_tensor[: int(len(train_data_tensor) * TRAIN_SPLIT)]\n",
    "val_data = train_data_tensor[int(len(train_data_tensor) * TRAIN_SPLIT) :]\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embed, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_embed = n_embed\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embeddings = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        self.queries = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.keys = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.values = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.proj = nn.Linear(head_size, n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size) \n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        token_emb = self.token_embeddings(x)  # (B, T, C)\n",
    "        position_emb = self.positional_embeddings(\n",
    "            torch.arange(T)\n",
    "        )  # (T, C)\n",
    "\n",
    "        x = token_emb + position_emb\n",
    "\n",
    "        q = self.queries(x)  # (B, T, C)\n",
    "        k = self.keys(x)  # (B, T, C)\n",
    "        v = self.values(x)  # (B, T, C)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * self.n_embed **-0.5  #  (B, T, T)\n",
    "        # In the case of an encoder, this masked_fill will not be here, so that all tokens can freely communicate with each_other\n",
    "        wei = wei.masked_fill(\n",
    "            self.tril[:T, :T] == 0, float(\"-inf\")\n",
    "        )  # tril will be braodcaseted in the following way\n",
    "        # (B, T, T) and  (T, T) need broadcasting\n",
    "        # Pytorch will right align them\n",
    "        # (B, T, T)\n",
    "        #    (T, T)\n",
    "        # And it will then add a new dim\n",
    "        # (B, T, T)\n",
    "        # (1, T, T)\n",
    "        # and then broadcast\n",
    "        # (B, T, T)\n",
    "        # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # Use the values instead of default private token encodings\n",
    "        x = wei @ v # (B, T, T) @ (B, T, C)\n",
    "\n",
    "        x = self.proj(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            idx_cond = idx[\n",
    "                :, -self.block_size :\n",
    "            ]  # Because we can only pass in the last block, This is where the context window limit comes from for transformer models\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e95cacdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embeddings): Embedding(65, 32)\n",
       "  (positional_embeddings): Embedding(8, 32)\n",
       "  (queries): Linear(in_features=32, out_features=16, bias=False)\n",
       "  (keys): Linear(in_features=32, out_features=16, bias=False)\n",
       "  (values): Linear(in_features=32, out_features=16, bias=False)\n",
       "  (proj): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BigramLanguageModel(VOCAB_SIZE, BLOCK_SIZE, N_EMBED, HEAD_SIZE).to(DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c76b6e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff6b8d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.1742, grad_fn=<NllLossBackward0>), torch.Size([32, 65]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, loss = model(X, y)\n",
    "loss, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b25ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works! Integrating this to the bigram file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc990285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " u Slui3NHyM:\n",
      ".Rb;&l:UO?.&NVCd?FZfRCdKW?Wy? W;iReZE!Tmny!v'\n",
      "3KhY,YFFEoORgEHkYnJQIi;q-HqPoG!'lKNvyNa3iAtbzu hzamCKBzNKUu;3Y\n",
      "h?JKTS; $WZbNniYri vKRjj\n",
      "NHr?jquCn!JKn$sCLf-AI.L\n",
      "YCXDaF&UxngT,nqkCHPQbBvxsj::YU!PjlGMBM-fKR sZJ;;KktpPdVhmnI&E? x-CFDvUJB,TJs$q -VnLCAfvV,xLWFvmvHOh!EGAsB3BsWeVlJDRJUUnbaYQ:hOR-KJ-AtOzrGtGqGZ?&WcsWvLYOBe\n",
      "lVZMStd'kdFjc&emF-OdoeEk.ewwqQQJ,WDmJvxU?G&ejxMaZWrE 'lhu,.YMeBOYAUV!BDbFjoCHafxz.zWe-!CI?YgXgtDWDOsfc?vjqHVwl'uouiNoqObKsqDGJmCgNHo'F:ik&qwn?rLluJVhbGR\n",
      ".-'uxHv,-vb;nvpr-K:J;s&PhDSwSzitGG?lrRWbnGXQ!yrHhFsu.\n",
      "Xd3hWRJwM$Mh:KeQAsDktgVs$iFMRgrXz$J'$MxpKsxL;hLGEw!.oKSaqXZuO$k,?KvrwvK;lwv\n",
      "yNHqrenHaiEt-YaV:WSmflN-votidMOgGoyHaL3RLirfa!IZ MX-jtsWtIlkhZbfqpF-?JIUPkV,Q:$usmN&GxI ewZstkQeKUBZ-SQD:CU3Jr':-UhCKcjMijDT\n",
      "dWL,RrGoS$YYArdTnn VVNs;C-L\n",
      "xI\n",
      "RmM&ZZ Xqf\n",
      "JoWj'LwjlkxvFaJRLUNM 3OHjP\n",
      "Bsyrfx;ibIym:vQ?.N$Z'wJ$zb PTlmc$O:IDwISGl,Qjpq?XAjXfRztFIL&UftMPSjywjoRfl3e!RC\n",
      "LK\n",
      "mJb ;gIUqalBpK'EoDXJs3iVcQBH?33\n",
      "\n",
      "Ycf?hax':McnkLplTgfiWl$rQCVzkJ YQt!ZpiTT$L JJT'Yvtrw:pfw&WRRZLAAkSiRbLnrkeMGRmh.U\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    decode(\n",
    "        model.generate(torch.ones(1, 1, dtype=torch.long), MAX_NEW_TOKENS)[0].tolist()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023fcc4",
   "metadata": {},
   "source": [
    "# Multi-head Attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f852d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All we need to do to implement multi-head-attention is to rewrite the bigram-language-model--with-self-attention file such that the language model and the head are separate classes\n",
    "# And then we run miltiple heads in parallel, and then use scaled do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001581d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8313f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc7280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetuning-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
