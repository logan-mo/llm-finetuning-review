{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7504ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "TEXT_DATA_PATH = \"tiny_shiekspear.txt\"\n",
    "with open(TEXT_DATA_PATH, \"r\") as file:\n",
    "    text_data = file.read()\n",
    "    \n",
    "print(text_data[:500])  # Print the first 500 characters of the text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6625b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text_data)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "print(f\"Unique characters: {''.join(chars)}\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9366c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(text):\n",
    "    return [stoi[c] for c in text]\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([itos[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71ed1c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [46, 43, 50, 50, 53, 2]\n",
      "Decoded: hello!\n"
     ]
    }
   ],
   "source": [
    "encoded = encode(\"hello!\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "decoded = decode(encoded)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa4f6129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to torch tensor of encoded text and print\n",
    "import torch\n",
    "\n",
    "train_data_tensor = torch.tensor(encode(text_data), dtype=torch.long)\n",
    "train_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1a70d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tensor[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c63aad80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b97e0b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and validation split\n",
    "TRAIN_SPLIT = 0.9\n",
    "train_data = train_data_tensor[:int(len(train_data_tensor)*0.9)]\n",
    "val_data = train_data_tensor[int(len(train_data_tensor)*0.9):]\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "644abf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block indices: [18, 47, 56, 57, 58, 1, 15, 47, 58]\n"
     ]
    }
   ],
   "source": [
    "# Box size = context window\n",
    "# visualize a block of text (size = block_size + 1)\n",
    "\n",
    "BLOCK_SIZE = 8\n",
    "block = train_data[:BLOCK_SIZE + 1]\n",
    "print(f\"Block indices: {block.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bd67222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "For an input block: [18, 47, 56, 57, 58, 1, 15, 47]\n",
      "For input: [18], Next token would be: 47\n",
      "For input: [18, 47], Next token would be: 56\n",
      "For input: [18, 47, 56], Next token would be: 57\n",
      "For input: [18, 47, 56, 57], Next token would be: 58\n",
      "For input: [18, 47, 56, 57, 58], Next token would be: 1\n",
      "For input: [18, 47, 56, 57, 58, 1], Next token would be: 15\n",
      "For input: [18, 47, 56, 57, 58, 1, 15], Next token would be: 47\n",
      "For input: [18, 47, 56, 57, 58, 1, 15, 47], Next token would be: 58\n"
     ]
    }
   ],
   "source": [
    "# For one given block of text, we generate all possible continuations from the start\n",
    "# This gives us block_size examples that we can use to train the model\n",
    "\n",
    "X = train_data[: BLOCK_SIZE]\n",
    "y = train_data[1:BLOCK_SIZE+1] # y is just train data offset by one, as the task of the model is just to predict the next token given the previous sequence of tokens (or just last token in case of a bigram model)\n",
    "\n",
    "print(f\"For an input block: {X.tolist()}\")\n",
    "for t in range(BLOCK_SIZE):\n",
    "    print(f\"For input: {X[:t+1].tolist()}, Next token would be: {y[t]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62e186ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For an input block: [53, 44, 1, 59, 57, 10, 1, 58]\n",
      "For input: [53], Next token would be: 44\n",
      "For input: [53, 44], Next token would be: 1\n",
      "For input: [53, 44, 1], Next token would be: 59\n",
      "For input: [53, 44, 1, 59], Next token would be: 57\n",
      "For input: [53, 44, 1, 59, 57], Next token would be: 10\n",
      "For input: [53, 44, 1, 59, 57, 10], Next token would be: 1\n",
      "For input: [53, 44, 1, 59, 57, 10, 1], Next token would be: 58\n",
      "For input: [53, 44, 1, 59, 57, 10, 1, 58], Next token would be: 46\n"
     ]
    }
   ],
   "source": [
    "# To generate a batch of example tensor using the block code above:\n",
    "random_idx = torch.randint(0, len(train_data) - BLOCK_SIZE - 1, (1,)).item()\n",
    "X = train_data[random_idx: random_idx+BLOCK_SIZE]\n",
    "y = train_data[random_idx+1:random_idx+BLOCK_SIZE+1] # y is just train data offset by one, as the task of the model is just to predict the next token given the previous sequence of tokens (or just last token in case of a bigram model)\n",
    "\n",
    "print(f\"For an input block: {X.tolist()}\")\n",
    "for t in range(BLOCK_SIZE):\n",
    "    print(f\"For input: {X[:t+1].tolist()}, Next token would be: {y[t]}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "84cd104f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42, 39, 63,  0, 13, 57,  1, 47],\n",
      "        [ 1, 21,  1, 41, 39, 52, 52, 53],\n",
      "        [ 1, 61, 47, 50, 50,  1, 54, 56],\n",
      "        [46, 43,  1, 49, 47, 52, 45,  1]])\n",
      "tensor([[39, 63,  0, 13, 57,  1, 47, 57],\n",
      "        [21,  1, 41, 39, 52, 52, 53, 58],\n",
      "        [61, 47, 50, 50,  1, 54, 56, 39],\n",
      "        [43,  1, 49, 47, 52, 45,  1, 46]])\n"
     ]
    }
   ],
   "source": [
    "# Stacking\n",
    "# To generate a batch of example tensor using the block code above:\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "indices = torch.randint(0, len(train_data) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n",
    "X = torch.stack([train_data[idx: idx+BLOCK_SIZE] for idx in indices])\n",
    "y = torch.stack([train_data[idx+1: idx+BLOCK_SIZE+1] for idx in indices])\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f11200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input tensor: tensor([42]), Target is: 39\n",
      "For input tensor: tensor([42, 39]), Target is: 63\n",
      "For input tensor: tensor([42, 39, 63]), Target is: 0\n",
      "For input tensor: tensor([42, 39, 63,  0]), Target is: 13\n",
      "For input tensor: tensor([42, 39, 63,  0, 13]), Target is: 57\n",
      "For input tensor: tensor([42, 39, 63,  0, 13, 57]), Target is: 1\n",
      "For input tensor: tensor([42, 39, 63,  0, 13, 57,  1]), Target is: 47\n",
      "For input tensor: tensor([42, 39, 63,  0, 13, 57,  1, 47]), Target is: 57\n",
      "For input tensor: tensor([1]), Target is: 21\n",
      "For input tensor: tensor([ 1, 21]), Target is: 1\n",
      "For input tensor: tensor([ 1, 21,  1]), Target is: 41\n",
      "For input tensor: tensor([ 1, 21,  1, 41]), Target is: 39\n",
      "For input tensor: tensor([ 1, 21,  1, 41, 39]), Target is: 52\n",
      "For input tensor: tensor([ 1, 21,  1, 41, 39, 52]), Target is: 52\n",
      "For input tensor: tensor([ 1, 21,  1, 41, 39, 52, 52]), Target is: 53\n",
      "For input tensor: tensor([ 1, 21,  1, 41, 39, 52, 52, 53]), Target is: 58\n",
      "For input tensor: tensor([1]), Target is: 61\n",
      "For input tensor: tensor([ 1, 61]), Target is: 47\n",
      "For input tensor: tensor([ 1, 61, 47]), Target is: 50\n",
      "For input tensor: tensor([ 1, 61, 47, 50]), Target is: 50\n",
      "For input tensor: tensor([ 1, 61, 47, 50, 50]), Target is: 1\n",
      "For input tensor: tensor([ 1, 61, 47, 50, 50,  1]), Target is: 54\n",
      "For input tensor: tensor([ 1, 61, 47, 50, 50,  1, 54]), Target is: 56\n",
      "For input tensor: tensor([ 1, 61, 47, 50, 50,  1, 54, 56]), Target is: 39\n",
      "For input tensor: tensor([46]), Target is: 43\n",
      "For input tensor: tensor([46, 43]), Target is: 1\n",
      "For input tensor: tensor([46, 43,  1]), Target is: 49\n",
      "For input tensor: tensor([46, 43,  1, 49]), Target is: 47\n",
      "For input tensor: tensor([46, 43,  1, 49, 47]), Target is: 52\n",
      "For input tensor: tensor([46, 43,  1, 49, 47, 52]), Target is: 45\n",
      "For input tensor: tensor([46, 43,  1, 49, 47, 52, 45]), Target is: 1\n",
      "For input tensor: tensor([46, 43,  1, 49, 47, 52, 45,  1]), Target is: 46\n"
     ]
    }
   ],
   "source": [
    "# This X and y batch are both (B,T) matrices, \n",
    "#   where the B dim is the blocks across a BATCH\n",
    "#   T dim is across a block through TIME\n",
    "# For any given block Bx, for any given sequence Bx[start:end], the next token in the sequence would be By[end]\n",
    "for block in range(BATCH_SIZE):\n",
    "    for t in range(BLOCK_SIZE):\n",
    "        print(f\"For input tensor: {X[block, :t+1]}, Target is: {y[block, t]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6afaa733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[42,  1, 58, 46, 43, 51,  1, 41],\n",
       "         [53, 50, 43, 52, 58,  6,  1, 39],\n",
       "         [53, 51, 43,  1, 54, 47, 58, 63],\n",
       "         [56, 53, 54, 43,  7, 58, 56, 47]]),\n",
       " tensor([[ 1, 58, 46, 43, 51,  1, 41, 53],\n",
       "         [50, 43, 52, 58,  6,  1, 39, 52],\n",
       "         [51, 43,  1, 54, 47, 58, 63,  8],\n",
       "         [53, 54, 43,  7, 58, 56, 47, 41]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get batch -> x, y for a random batch (of blocks)\n",
    "# Note 1\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    indices = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    X = torch.stack([data[idx: idx+block_size] for idx in indices])\n",
    "    y = torch.stack([data[idx+1: idx+block_size+1] for idx in indices])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dce21b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram language model that uses nn.Embedding to generate Logits, cross entropy loss ### Comeback and try making a trigram model too\n",
    "# Check dimenstionality of input and output\n",
    "# Note 2\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_lookup_table = nn.Embedding(input_vocab_size, output_vocab_size)\n",
    "\n",
    "        # nn.embedding is essentially a matrix,\n",
    "        # I don't know how it works\n",
    "        # But here's my guess\n",
    "        # It is a (input_values, output_values) matrix, where the input token equals the possible input variation\n",
    "        # In a bigram model, where we look at one past character to predict the next one, input_values = output_values = charater_set_length\n",
    "        # In a trigram model, where we look at two past characters to predict on next character, output_value = charater_set_length, input_values = charater_set_length ** 2\n",
    "        # In this trigram model, the embedding matric would me (charater_set_length ** 2, charater_set_length)\n",
    "        # In LLM, I am guessing this is a (vocab_size, vocab_size) matrix\n",
    "        \n",
    "        \n",
    "        # Now as to what it does, the forward pass using this as a lookup table of probabilities,\n",
    "        # For any given row, we treat the column values as the probabilities of which output token should come next\n",
    "        # This way when we back prop, we are optimizing for the lookup table to resemble a probability distribution matrix as done is makemore\n",
    "        \n",
    "        # Apparently I am precisely correct about how nn.Embedding does. This might as well have been implemented with a normal (input_values, output_values) Tensor with gradients and it would've worked just the same\n",
    "\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        # x is shape (B, T), where B is batch size, T is time steps (or block size)\n",
    "        x = self.embedding_lookup_table(idx)\n",
    "        # x is now (B, T, C) where C is the character set size (vocab size)\n",
    "        if not target:\n",
    "            return x, None\n",
    "        return x , f.cross_entropy()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "819d0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the forward pass to work, cross_entropy expects input, target. BUTTTTT Andrej mentioned something the input from B,T,C to B,C, T, will run and test\n",
    "model = BigramLanguageModel(VOCAB_SIZE, VOCAB_SIZE)\n",
    "\n",
    "X, y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "19255831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass one token, which is the last token of the input sequence, because this is a bigram model\n",
    "# But the input sequnce in a moving window, as we printed above\n",
    "# So the actual lookup indices should be a (B*BLOCK_SIZE, C)\n",
    "# But I don't know how to implement that, so I am justs going to get one example per block for the sack of making the forward pass work\n",
    "\n",
    "print(model.embedding_lookup_table(X[:, -1]).shape), print(y[:, -1].shape)\n",
    "# This gives me (BATCH_SIZE, VOCAB_SIZE), where the 65 is the vocab size logtis that I am assuming are the probabilites of what the next token should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e65dc232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.distributions.Polynomial() # I don't know how this will work in pytorch, but I am guessing after we get the embedding logits,\n",
    "# We we sample for a distribution of that embedding, which will give us the index of the next token, that will be our actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0100a1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5493, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = model.embedding_lookup_table(X[:, -1])\n",
    "target = y[:, -1]\n",
    "F.cross_entropy(seq, target)\n",
    "\n",
    "# This works, but again, this is (B, C) against (B), I have no idea how to make this work in (B, T, C) against (B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4632ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oh, I keep struggling because I keep trying to write the most general solution ever\n",
    "# This is a bigram model, model.embedding_lookup_table(X) for a (B, T) matrix will give me (B, T, C). This is because in bigram models, only the current token is needed to predict the next token\n",
    "# No need to do the whole generate completel sequence thing. This will not work for Trigram, or atleast not exactly like this\n",
    "print(model.embedding_lookup_table(X).shape), print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d5cd67fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 65], got [4, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m seq = model.embedding_lookup_table(X)\n\u001b[32m      2\u001b[39m target = y\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# F.cross_entropy(seq, target) given an error, RuntimeError: Expected target size [4, 65], got [4, 8]\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# I am guessing this is because, my target is indices, not probablity values to match with. So cross entropy thinks that in my input (B, T, C). T is the probabilities, and C is the classes.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# essentially (batch, probability, class)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# So for this I will transform seq but only the two inner dimensions, so that is goes from (B, T, C) to (B, C, T)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github_repos_windows\\llm-finetuning-review\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected target size [4, 65], got [4, 8]"
     ]
    }
   ],
   "source": [
    "seq = model.embedding_lookup_table(X)\n",
    "target = y\n",
    "F.cross_entropy(seq, target)\n",
    "\n",
    "# F.cross_entropy(seq, target) given an error, RuntimeError: Expected target size [4, 65], got [4, 8]\n",
    "# I am guessing this is because, my target is indices, not probablity values to match with. So cross entropy thinks that in my input (B, T, C). T is the probabilities, and C is the classes.\n",
    "# essentially (batch, probability, class)\n",
    "# So for this I will transform seq but only the two inner dimensions, so that is goes from (B, T, C) to (B, C, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fca8e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 65, 8])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.transpose(-2, -1).shape\n",
    "# Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "920f0cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8080, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(seq.transpose(-2, -1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24363c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boom!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e8f404cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), torch.Size([]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_lookup_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        # This is how I did it:\n",
    "        # x = self.embedding_lookup_table(idx)\n",
    "        # if target is None:\n",
    "        #     return x, None\n",
    "        # return x , F.cross_entropy(x.transpose(-2, -1), target)\n",
    "\n",
    "        # this is how Andrej did it\n",
    "        logits = self.embedding_lookup_table(idx)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = target.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def my_forward(self, idx, target=None):\n",
    "        x = self.embedding_lookup_table(idx)\n",
    "        if target is None:\n",
    "            return x, None\n",
    "        return x , F.cross_entropy(x.transpose(-2, -1), target)\n",
    "\n",
    "X, y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "model = BigramLanguageModel(VOCAB_SIZE)\n",
    "y_pred , loss = model.forward(X, y)\n",
    "y_pred.shape, loss\n",
    "\n",
    "# Well my and Andrej's methods are giving different results.... welp lemme chatgpt what is the differenct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6cfea00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), tensor(4.6674, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred , loss = model.forward(X, y)\n",
    "y_pred.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6ef19755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 65]), tensor(4.6674, grad_fn=<NllLoss2DBackward0>))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred , loss = model.my_forward(X, y)\n",
    "y_pred.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc287871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea what changed (Edit: I do now, different batch in the first run), but by implementing them both, very clearly they are matehmatically equivalent\n",
    "# I'm a genius!\n",
    "# Look Ma! No hands!\n",
    "# Now let's cleanup, and implement a max_token limited generate function to let the model babble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3051b684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 65]), tensor(4.3515, grad_fn=<NllLoss2DBackward0>))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_lookup_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        x = self.embedding_lookup_table(idx)\n",
    "        if target is None:\n",
    "            return x, None\n",
    "        return x , F.cross_entropy(x.transpose(-2, -1), target)\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): # idx is the start seed token\n",
    "        # idx is [B, T], ideally where T=1, so a batch of input seed tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx) # Generate logits for what should come next for each B\n",
    "            logits = logits[:, -1,:]\n",
    "            probs = F.softmax(logits, dim=1) # We softmax the logits to convert them into probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # We pick the next token from a distribution \n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # We concatenate the newly generated token to the end of the starting sequence\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "X, y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "model = BigramLanguageModel(VOCAB_SIZE)\n",
    "y_pred , loss = model.forward(X, y)\n",
    "y_pred.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "057aaa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hm!pBQGR\n",
      "h:\n",
      "kqX3I!oMbB&TSsDMIizI!rF\n",
      "xjpAwI-Ry.GjLGwatYyBquxjFOBYZeOEYFObrQOB&Lr-fK&tu-yTeB3U,SfnjGYF&'GN!$OBTZ 3V hfoQOBTpd'tpA-&w-ctxbmDdW:W$I'cHdlfPJolw!Ii.CnuB!ooCuWoqM?R-!B.anOB3o$pNLjYF'EEYUtPnM;OwNEJSdkMt3u'Z;OBLTSFNDk:v,;&fk:vpvrhiEAx.ddoyc!jAy.'tRY;OBky $Vv;UpdjMtxSuTthbaTH oZhy!?LQY;OMc ;OB!Ikgx\n",
      "&TOBvEoB&FKoleMGYVK\n",
      "G:gPb-yGthAqfoU,OBTZelR&OM?t&MUvLqN'vvbsv.Gs;hUNJ\n",
      "ZTlSBzgpH!J&DS&U,qUvFAo3ErGgjai.\n",
      "v SjfI3Id;,sDYfPxVQF'.yd'JSuk:wICpO BOJApdPOgwjM?W:t'Qy.&hank:VXbPMI!Wsm-,Krhcl?VY$aUu?ulyo&g'P,Lin3JGjdXYXNE$lfTI'Q;SuvlSdBu!glbf'w;jGR3MlwmbMbBOltzVyA,SB-BO;vJ3ILrypDkAelgD'fo &:.FVnD:anjrhnM3lw!,,weuVWgKoRRqahynDYEQTy sd-lw3$fZfjCux?qdkZm!-sszzmTRvvi$n;\n",
      "Zx\n",
      "Q?iRPahuQ;OBgxwevAbNkYFZuF!m!vb\n",
      ";OBhEOB3Iu tZtuDeYYZfr-GtwbaqfgMZANkazuVXYFsvv.FPFIV'wNEiSXBth,Bvm,tD.PRyUbowI$NSPHD\n",
      ";cK\n",
      "Rq-' nCHvb' xXpavJ$r-ouA3JYWbMGAHe' yLNYUc\n",
      "khcscsPnBHminrh,SLQTE&tZnNEoe?QFjrfoM;FsV3tzgX:;OBYkAYByNE;\n",
      "Wk!AH$qvbIdXAU\n",
      "$z.EAYvbE'hiSPKNkkgqpe'tsxqZbGv,Sdg:gvTbOUbTLyGfYxFTlxpELw?EYesFZOcMdiY3lDJVeSuN'eYFrF,wftfi\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros(1, 1, dtype=torch.long), 1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "387120e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rubbish, but expected because the model is untrained. Let's train!\n",
    "model = BigramLanguageModel(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "70c10f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4207818508148193\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch(train_data, batch_size, BLOCK_SIZE)\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1270ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " m fo, g thr.\n",
      "DWhirdrind Vaxce? fathe ar ie VEThe fo in my: Whe GEENESSThat\n",
      "D:\n",
      "Ithay ee s e:\n",
      "Singlithe angrs,\n",
      "Yorou\n",
      "\n",
      "GHarierel magho than uryootofrdin, m me e blofonof.\n",
      "Thers onk thought thearessk, nang le;\n",
      "Finceilerisbu orth I fer s thistheaven.\n",
      "A:\n",
      "Waigsuralairarwhinomanstt se fongheenon fr. d t berve'eck y loueldind hirelaimucknar I thoupoy N:\n",
      "\n",
      "Yo umy sw,\n",
      "thaecty yo won, alirshigorthe storirn hinto 'lise hbbouttlthe,\n",
      "USh-the?\n",
      "An sthe ly.\n",
      "\n",
      "BULTerin Wes p thonwaltovouberon f siery sirofodsucthe dre ows y ot then.\n",
      "uterorthiviggobour an.\n",
      "\n",
      "\n",
      "Shasifoofoinoutitins hindi'd hin iusithathaped fullalofo, he'sthote y'ORINGHalle bu hech ten Gllilst my, marron'shehee,\n",
      "Priate theses t yowich ve lerenerer t pue\n",
      "\n",
      "\n",
      "ID nwac foushendorof home th at.\n",
      "ARO ear gin bl hese,\n",
      "FOFiree\n",
      "\n",
      "ABrine llo knge Fob ashiabers tin'sowh ftof o, M:\n",
      "Helo lthese ISe:\n",
      "Ork theth hay t:\n",
      "YOfren foend horsofod.\n",
      "Tit cr or d d pun:\n",
      "METINRKERDanngeawaclita nn'su orghad I waighefin!\n",
      "NGENond 'lar ougorir my ke yor\n",
      "COLONTotho te d,\n",
      "QUS:\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.ones(1, 1, dtype=torch.long), 1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good enough, let'smove this over to transformer_dev.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetuning-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
